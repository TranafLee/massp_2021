{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging vs. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "\n",
    "Bagging = Boostrap Averaging\n",
    "\n",
    "- Boostrap:\n",
    "\n",
    "    - Suppose we have 1000 data points → D = 1000\n",
    "    - We get 15 datasets, each has 700 randomly chosen data points from our pool of 1000 data points → B = 15, n = 700\n",
    "    - Learning algorithm gives B decision functions: $\\hat{f_1}(x)$, $\\hat{f_2}(x)$, ..., $\\hat{f_B}(x)$\n",
    "\n",
    "- Averaging:\n",
    "    - We fix some particular x_0\n",
    "    - Then we have: $\\hat{f_{avg}}(x_0)$ = $\\frac{1}{B} \\sum_{b=1}^{B} \\hat{f_{b}}(x_0)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Improving weak learners by building weak learn sequentially (i.e., the next weak learning is built upon the previous weak learner with some improvement)\n",
    "\n",
    "- Boosting Steps: \n",
    "    - A weak learner: a classifier that does better than random\n",
    "    - Combine a set of weak learner to form a single classifier that makes accurate predictions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting - Adaboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Binary classification: $Y = \\{-1, 1\\}$\n",
    "- Training set: $D = \\{ \\{x_1, y_1\\}, \\{x_2, y_2\\}, ..., \\{x_n, y_n\\}\\}$\n",
    "- Weights: $W = \\{ w_1, w_2, w_3, ..., w_n \\}$\n",
    "- At the begining $m = 1$, $w_1 = w_2 =... w_n = \\frac{1}{n}$\n",
    "\n",
    "- For each round m = 1...M, we have a base learner $G_m(x)$\n",
    "\n",
    "    - Calculate error: \n",
    "$err_m = \\frac{\\sum_{i=1}^{n} (w_i * terror_i)}{\\sum_{i=1}^{n} (w_i)} = \\frac{A}{B} $\n",
    "\n",
    "        Where:\n",
    "$$\n",
    "\\begin{align}\n",
    "A = \\sum_{i=1}^{n} (w_i * terror_i)\n",
    "\\\\\n",
    "B = \\sum_{i=1}^{n} (w_i)\n",
    "\\\\\n",
    "terror_i = 1 * ( y_i \\# G_m(x_i) )\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "    - Calculate rate of the learner:\n",
    "$\\alpha_m = ln \\frac{1 - err_m}{err_m}$ \n",
    "\n",
    "    - Update weight $w_i$: -- The idea is to give more weights to the wrongly classified data\n",
    "$w_i = w_i * e^{\\alpha_m} = w_i * \\frac{1 - err_m}{err_m}$\n",
    "\n",
    "\n",
    "- After M rounds, we have \n",
    "    - M classifiers: $G_1(x), G_1(x), ..., G_M(x)$\n",
    "    - M rates of the learners: $\\alpha_1, \\alpha_2, ..., \\alpha_M$\n",
    "    \n",
    "- Final classifier: $G(x) = sign[\\sum_{m=1}^{M} (\\alpha_m * G_m(x))] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
