{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient computation with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def sigmoid_der(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def relu(x):\n",
    "    # Fill code here for relu(x) = max(0,x)\n",
    "    pass\n",
    "def relu_der(x):\n",
    "    # Fill code here for derivative of relu\n",
    "    pass\n",
    "\n",
    "def linear(W,x,beta):\n",
    "    return np.matmul(W,x).reshape(-1,1)+beta\n",
    "\n",
    "def dfXdX_func(W,x,beta,act_der):\n",
    "    # Derivative of f(X_{k-1}) with respect to X_{k-1}\n",
    "    return np.matmul(np.diag(act_der(linear(W,x,beta)).ravel()), \n",
    "                     W).reshape(W.shape)\n",
    "\n",
    "def dfXdbeta_func(W,x,beta,act_der):\n",
    "    # Derivative of f(X_{k-1}) with respect to beta_k\n",
    "    dfXdbeta = np.diag(act_der(np.matmul(W,x)+beta).ravel())\n",
    "    return dfXdbeta\n",
    "\n",
    "def dfXdWi_func(W,x,beta,act_der,i):\n",
    "    # Derivative of f(X_{k-1}) with respect to row i of W_k\n",
    "    D = np.diag(act_der(np.matmul(W,x)+beta).ravel())\n",
    "    Xi = np.zeros(W.shape)\n",
    "    Xi[i,:] = x.T\n",
    "    return np.matmul(D,Xi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.02614442699996289\n",
      "dl/dX0:  [[-0.05246989  0.28237541]]\n",
      "dl/dX2:  [[1.46464132]]\n",
      "dl/dX1:  [[-0.47585847  0.0634478   0.15861949]]\n",
      "dl/dW1: [[-0.05948231  0.05948231]\n",
      " [ 0.00691248 -0.00691248]\n",
      " [ 0.0068411  -0.0068411 ]]\n",
      "dl/dbeta1: [-0.11896462  0.01382496  0.01368219]\n",
      "dl/dW2: [[0.15861949 0.10177702 0.28699041]]\n",
      "dl/dbeta2: [0.31723898]\n"
     ]
    }
   ],
   "source": [
    "c = [2,3,1]\n",
    "X0 = np.array([0.5,-0.5]).reshape(c[0],1)\n",
    "W1 = np.array([[0.5,-2.5],[1.2,0.1],[-0.7,-1.2]]).reshape(c[1],c[0])\n",
    "beta1 = np.array([-1.5,-1.3,2.0]).reshape(c[1],1)\n",
    "W2 = np.array([-1.5, 0.2, 0.5]).reshape(c[2],c[1])\n",
    "beta2 = np.array([1.0]).reshape([c[2],1])\n",
    "W = {1:W1,2:W2}\n",
    "beta = {1:beta1, 2:beta2}\n",
    "X = {0:X0}\n",
    "\n",
    "activation = sigmoid\n",
    "activation_der = sigmoid_der\n",
    "K = 2\n",
    "\n",
    "# compute X\n",
    "for i in range(1,K+1):\n",
    "    X[i] = activation(linear(W[i],X[i-1],beta[i]))\n",
    "\n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# compute df(X)/d(X)\n",
    "dfXdX = {}\n",
    "for i in range(2):\n",
    "    dfXdX[i] = dfXdX_func(W[i+1],X[i],beta[i+1],activation_der)\n",
    "\n",
    "# compute dl/dX\n",
    "dldX = {2:1/X[2]}\n",
    "for i in range(K-1,-1,-1):\n",
    "    dldX[i] = np.matmul(dldX[i+1],dfXdX[i])\n",
    "\n",
    "# compute dl/dW and dl/dbeta\n",
    "dldW = {}\n",
    "dldbeta = {}\n",
    "for i in range(K,0,-1):\n",
    "    dldbeta[i] = np.matmul(dldX[i],\n",
    "                           dfXdbeta_func(W[i],X[i-1],beta[i],activation_der)\n",
    "                          ).ravel()\n",
    "    dldW[i] = np.array([np.matmul(dldX[i], \n",
    "                                  dfXdWi_func(W[i],X[i-1],beta[i],activation_der,j)\n",
    "                                 ).ravel()\n",
    "                        for j in range(W[i].shape[0])])\n",
    "\n",
    "print('Time: ', timeit.default_timer() - start)  \n",
    "\n",
    "print(f'dl/dX0: ', dldX[0])\n",
    "print(f'dl/dX2: ', dldX[2])\n",
    "print(f'dl/dX1: ', dldX[1])\n",
    "print('dl/dW1:', dldW[1])\n",
    "print('dl/dbeta1:', dldbeta[1])\n",
    "print('dl/dW2:', dldW[2])\n",
    "print('dl/dbeta2:', dldbeta[2])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient computation with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\r\n",
      "Requirement already satisfied: torch==1.9.0+cpu in /home/huy/anaconda3/lib/python3.7/site-packages (1.9.0+cpu)\r\n",
      "Requirement already satisfied: torchvision==0.10.0+cpu in /home/huy/anaconda3/lib/python3.7/site-packages (0.10.0+cpu)\r\n",
      "Requirement already satisfied: torchaudio==0.9.0 in /home/huy/anaconda3/lib/python3.7/site-packages (0.9.0)\r\n",
      "Requirement already satisfied: typing-extensions in /home/huy/anaconda3/lib/python3.7/site-packages (from torch==1.9.0+cpu) (3.10.0.0)\r\n",
      "Requirement already satisfied: pillow>=5.3.0 in /home/huy/anaconda3/lib/python3.7/site-packages (from torchvision==0.10.0+cpu) (6.2.0)\r\n",
      "Requirement already satisfied: numpy in /home/huy/anaconda3/lib/python3.7/site-packages (from torchvision==0.10.0+cpu) (1.17.2)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install torch==1.9.0+cpu torchvision==0.10.0+cpu torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "c = [2,3,1]\n",
    "X0 = Variable(torch.Tensor(np.array([0.5,-0.5]).reshape(c[0],1)), requires_grad=True)\n",
    "W1 = Variable(torch.Tensor(np.array([[0.5,-2.5],[1.2,0.1],[-0.7,-1.2]]).reshape(c[1],c[0])), requires_grad=True)\n",
    "beta1 = Variable(torch.Tensor(np.array([-1.5,-1.3,2.0]).reshape(c[1],1)), requires_grad=True)\n",
    "W2 = Variable(torch.Tensor(np.array([-1.5, 0.2, 0.5]).reshape(c[2],c[1])), requires_grad=True)\n",
    "beta2 = Variable(torch.Tensor(np.array([1.0]).reshape([c[2],1])), requires_grad=True)\n",
    "\n",
    "X1 = torch.sigmoid(torch.matmul(W1,X0)+beta1)\n",
    "X1.retain_grad()\n",
    "X2 = torch.sigmoid(torch.matmul(W2,X1)+beta2)\n",
    "X2.retain_grad()\n",
    "l = torch.log(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0.0027164640050614253\n",
      "dl/dX0:  tensor([[-0.0525],\n",
      "        [ 0.2824]])\n",
      "dl/dX1:  tensor([[-0.4759],\n",
      "        [ 0.0634],\n",
      "        [ 0.1586]])\n",
      "dl/dX2:  tensor([[1.4646]])\n",
      "dl/dW1:  tensor([[-0.0595,  0.0595],\n",
      "        [ 0.0069, -0.0069],\n",
      "        [ 0.0068, -0.0068]])\n",
      "dl/dbeta1:  tensor([[-0.1190],\n",
      "        [ 0.0138],\n",
      "        [ 0.0137]])\n",
      "dl/dW2:  tensor([[0.1586, 0.1018, 0.2870]])\n",
      "dl/dbeta2:  tensor([[0.3172]])\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "l.backward()\n",
    "print('Time: ', timeit.default_timer() - start)  \n",
    "print(f'dl/dX0: ', X0.grad)\n",
    "print(f'dl/dX1: ', X1.grad)\n",
    "print(f'dl/dX2: ', X2.grad)\n",
    "print(f'dl/dW1: ', W1.grad)\n",
    "print(f'dl/dbeta1: ', beta1.grad)\n",
    "print(f'dl/dW2: ', W2.grad)\n",
    "print(f'dl/dbeta2: ', beta2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
