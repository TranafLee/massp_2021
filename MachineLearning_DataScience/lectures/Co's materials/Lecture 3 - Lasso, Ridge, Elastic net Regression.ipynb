{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regularization\n",
    "- **Question:**\n",
    "    - What is regularization?\n",
    "    - Why do we need to regularize?\n",
    "  \n",
    "## 3.1 Bias and Variance Tradeoff\n",
    "### 3.1.1 Intuition\n",
    "-  Given a learnable objective function, Bias and Variance tradeoff is a property that constraints the effort trying to simultaneously minimize these two sources of error from generalizing beyond the training set.\n",
    "\n",
    "### 3.1.2 Bias error\n",
    "- **Intuition:** The bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting). Another way to think about it is if the error has no favorable direction or doesn't put much attentions on the training dataset, leading high total errors on training and testing sets.\n",
    "- **Example:** there are 2 scales A & B. My weight is 70kg. We conduct 4 trials of measuring on each scale\n",
    "    - Recordings on scale A: 71 - 72 - 71 -72\n",
    "    - Recordings on scale B: 75 - 65 - 80 -60\n",
    "- **Conclusion:** $\\mathbb{E}_A[W] = 71.5$ and $\\mathbb{E}_B[W] = 70$ so A is more accurate but biased toward 71.5 kg (possitive bias) and B is less accurate but unbiased (on average the measured weight is 70kg).\n",
    "- **Mathematical definition:** given an estimator $\\hat{\\theta}$ of the target statistic $\\theta$. The bias of $\\hat{\\theta}$ with respect to $\\theta$ is formulated as:\n",
    "$$Bias_D(\\hat{\\theta},\\theta) = \\mathbb{E}_{D}(\\hat{\\theta}) - \\theta$$\n",
    "With $\\mathbb{E}_{D}$ be the expected value over the possible $x$ observations under the distribution $D \\sim \\mathbb{P}(x|\\theta)$\n",
    "\n",
    "### 3.1.3 Variance error\n",
    "- **Intuition:** the variance error is an error which can be understood as “the mean of the deviation around the mean”, aka given a data point, variance captures the spread of the prediction. Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before. As a result, such models perform very well on training data but has high error rates on test data.\n",
    "- **Example:** with the sample configuration on bias error.\n",
    "    - Variance of scale A (exercise): low\n",
    "    - Variance of scale B (exercise): high\n",
    "- **Conclusion:** scale A has high bias but low variance and scale B has low variance - high bias.\n",
    "- **Mathematical definition:** given an estimator $\\hat{\\theta}$ of the target statistic $\\theta$. The variance of $\\hat{\\theta}$ with respect to $\\theta$ is formulated as:\n",
    "$$Var_D(\\hat{\\theta}) = \\mathbb{E}_D\\left[\\left(\\mathbb{E}_D[\\hat{\\theta}] - \\theta\\right)^2\\right]$$\n",
    "\n",
    "### 3.1.4 Mathematical Relationship\n",
    "- Given the error measure MSE (mean squared error/L2 loss function)\n",
    "$$\\begin{align*}\n",
    "L(\\theta,\\hat{\\theta}) &= \\mathbb{E}_D\\left[(\\hat{\\theta}-\\theta + \\mathbb{E}_D[\\hat{\\theta}] - \\mathbb{E}_D[\\hat{\\theta}] )^2\\right] \\\\\n",
    "&= (\\mathbb{E}_D[\\hat{\\theta}-\\theta)^2 + \\mathbb{E}_D[\\hat{\\theta}-\\mathbb{E}_D(\\hat{\\theta})]^2 \\text{ Prove this -exercise}\\\\\n",
    "&= Bias_D(\\hat{\\theta},\\theta)^2 + Var_D(\\hat{\\theta}) \n",
    "\\end{align*}$$\n",
    "\n",
    "## 3.2 Regularization\n",
    "- The idea of putting constraints on the functional space to enforce some properties on the predictor (preventing gradient explodes, smoother fitted hyperplane)\n",
    "\n",
    "### 3.2.1 Shrinkage Methods\n",
    "- **Question:** is unbiased estimator always good? why? why not?\n",
    "- **Example:** supposed that we achieved the best unbiased linear model which meant among all the unbiased models, ours has the lowest variance. However, it turns out that although we have unbiased predictions and minimal variance among all unbiased predictions, the variance can still be pretty large. One way to fix that is we introduce a little bit of biased and simultaneously reduce a significant amount of variance (why?). Doing this process carefully and elegantly, we can have a lower prediction error.\n",
    "- **Conclusion:** the following shrinkage technique does exactly that. They put constraint on the model to pull the parameters towards zero. Hence, the predictor will be biased (why?) and also have lower variance. This is a method of playing the trade off game between bias and variance.\n",
    "\n",
    "#### 1. Lasso (L1)\n",
    "- **Definition:**\n",
    "$$argmin_{\\beta}\\{\\frac{1}{N}||y-X\\beta||_2^2 + \\lambda_1||\\beta||_1\\}$$\n",
    "\n",
    "#### 2. Ridge (L2)\n",
    "- **Definition:**\n",
    "$$argmin_{\\beta}\\{\\frac{1}{N}||y-X\\beta||_2^2 + \\lambda_2||\\beta||_2\\}$$\n",
    "\n",
    "#### 3. Elastic net (L1 + L2)\n",
    "- **Definition:**\n",
    "$$argmin_{\\beta}\\{\\frac{1}{N}||y-X\\beta||_2^2 + \\lambda_1||\\beta||_1+\\lambda_2||\\beta||_2\\}$$\n",
    "\n",
    "#### 4. Tune the hyper-parameter lambda\n",
    "- Try \n",
    "\n",
    "### 3.2.3 Comparision\n",
    "#### The effect of minimizing L1/L2/L1+L2 penalty\n",
    "- **Lasso:**\n",
    "    - Dimension reduction/Sparsifying the parameters. (why? prove this)\n",
    "- **Ridge:**\n",
    "    - Reduce the impact of non-relevant features. For highly correlated feature, it shrinks the values of them towards each other.\n",
    "- **Elastic net:**\n",
    "    - Compromise the effect of both.\n",
    "    \n",
    "## 3.3 Exercise:\n",
    "\n",
    "### 3.3.1 \n",
    "- Solve the analytical solutions for lasso/ridge\n",
    "\n",
    "### 3.3.2\n",
    "- Add the regularizations on the linear regression code and see the differences in the parameter vector\n",
    "\n",
    "### 3.3.3 \n",
    "- Show the proof in section 3.1.4\n",
    "\n",
    "### 3.2.3\n",
    "- Proove the claims in section 3.2.3 (hint: check the gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
