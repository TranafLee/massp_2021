{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear Regression\n",
    "\n",
    "## 2.1 Introduction\n",
    "\n",
    "- Linear regression may be both the simplest and most popular among the standard tools to regression. Dating back to the dawn of the 19th century, linear regression flows from a few simple assumptions:\n",
    "    1. Linear relationship between independent variables $x$ and dependent variable $y$, in other words, $y$ can be expressed as a weighted sum of elements in $x$ plus some observational noise.\n",
    "    2. The noise follows a Gaussian distribution $Gaussian(\\mu,\\sigma)$\n",
    "    3. Constant variance (homoscedasticity). This means that different values of the response variable have the same variance in their errors, regardless of the values of the predictor variables (which means $\\sigma$ above should approximately be a constant value, not a random variable)\n",
    "\n",
    "## 2.2 Terminology and formulation\n",
    "\n",
    "- Given the dataset $D := \\{(x_i,y_i)\\}_{i=1}^n$ with the linear relationship assumption can be expressed as\n",
    "$$y_i = \\beta_0 + \\beta_1 x_i^1 + \\beta_2 x_i^2 + ... + \\beta_d x_i^d + \\epsilon_i = x_i^T \\beta + \\epsilon_i$$ with $\\beta_i \\in \\mathbb{R}$. \n",
    "For the pair of data $(y_i,x_i)$ and $x_i = [x_i^1,x_i^2,...,x_i^d]$ or equivalently, the independent variable $x$ has $d$ features. In the matrix form\n",
    "$$y = X\\beta + \\epsilon$$\n",
    "Where $y,\\epsilon,\\beta \\in \\mathbb{R}^n, X \\in\\mathbb{R}^{n\\times d}$\n",
    "- Note: $\\beta$ is representating for the linear relationship between $y,X$, not a set of learned parameters (because we include the noise in the function).\n",
    "\n",
    "## 2.3 Building the predictor\n",
    "\n",
    "- We define the model parameter as $\\beta = [\\beta_0,\\beta_1,...,\\beta_n]$, and the predicted dependent variable $\\hat{y}$. The linear model is defined as\n",
    "$$\\hat{y}_j = \\sum_{i=1}^n x_j^i \\cdot \\beta_i + \\beta_0$$\n",
    "$$\\hat{y} = X\\beta + \\beta_0$$\n",
    "\n",
    "- **Loss function**: in the previous lecture, we learn about the process of measuring the differences. FOr standard linear regression (you can have different type of linear regression based on the model architecture as well as loss function), the loss function indicates the mean of **Euclidean distance** between predicted - ground truth values\n",
    "\n",
    "$$\\mathbf{L}^i(D,\\beta) = \\frac{1}{2}||\\hat{y}^i-y^i||_2^2$$\n",
    "$\\mathbf{L}^i(D,\\beta)$ is the loss function for each pair, the general loss function over the dataset is the average of all the pairs\n",
    "$$\\mathbf{L}(D,\\beta)= \\frac{1}{2n}\\sum_{i=1}^n\\mathbf{L}^i(D,\\beta) = \\frac{1}{2n}\\sum_{i=1}^n(\\hat{y}^i-y^i)^2$$\n",
    "\n",
    "## 2.4 Optimization problem\n",
    "- Another name for linear regression is the process of finding the best fit line, so given our parameter space ($\\beta \\in \\mathbb{R}^d$) we have the optimization problem:\n",
    "$$\\hat{\\beta} = argmin_{\\beta} \\mathbf{L}(D,\\beta)$$\n",
    "- Intuition: we try to find the parameter set that produces the minimal error (loss function).\n",
    "\n",
    "## 2.5 Solving the problem\n",
    "\n",
    "### 2.5.1 Analytical solution\n",
    "\n",
    "- We can rewrite the loss function with the product operator\n",
    "$$\\begin{align}\\mathbf{L}(D,\\beta) &= ||X\\beta - y||^2 \\\\\n",
    "&= (X\\beta -y)^T(X\\beta -y) \\\\\n",
    "&= (\\beta^TX^T- y^T)(X\\beta -y) \\\\\n",
    "&= \\beta^TX^TX\\beta- \\beta^TX^Ty -y^TX\\beta + y^Ty \\end{align}$$\n",
    "- To assure that the solution is a minimizer, we have that the partial derivative $\\mathbf{L}(D,\\beta)$ with respect to the parameter $\\beta$ is 0.\n",
    "$$\\begin{align}\\frac{\\partial \\mathbf{L}(D,\\beta)}{\\partial \\beta} &= 2\\beta^TX^TX -2y^TX\\\\\n",
    "&= 0 \\\\\n",
    "\\rightarrow y^TX &= \\beta^TX^TX \\\\\n",
    "\\rightarrow X^Ty &= XX^T\\beta \\\\\n",
    "\\rightarrow \\beta &= (XX^T)^{-1}X^Ty\\end{align}$$\n",
    "- So the analytical solution is\n",
    "$$\\hat{\\beta} = (XX^T)^{-1}X^Ty $$\n",
    "- Note: we solve the equation fairly easily, the analytical solution can be achievable with respect to any differential loss function $\\mathbf{L} \\in C^1$.\n",
    "\n",
    "### 2.5.2 Inexact solution\n",
    "\n",
    "- **Minibatch Gradient Descent**: a batch is a set of $k$ pairs of training samples getting from the dataset, denoted as $B$. The update rule for Minibatch Gradient Descent is\n",
    "$$\\begin{align}\\beta_i &= \\beta_i  - \\frac{\\eta}{|B|}\\sum_{j\\in B}\\partial_{\\beta_i} \\mathbf{L}^i(D,\\beta) \\\\\n",
    "&= \\begin{cases}&\\beta_i  - \\frac{\\eta}{|B|}\\sum_{j\\in B}x_j(x_j\\cdot \\beta + \\beta_0 - y_j) \\text{ for $i\\neq0$} \\\\\n",
    "&\\beta_i  - \\frac{\\eta}{|B|}\\sum_{j\\in B}(x_j\\cdot \\beta +\\beta_0 - y_j) \\text{ for $i = 0$}\\end{cases}\\end{align}$$\n",
    "- **Termination condition**: because the update rule is applied for each batch of each epoch, we can either stop when iterating all the pre-specified number of epochs or loss value < some value $d$. \n",
    "\n",
    "### 2.5.3 Discussion\n",
    "\n",
    "- What to use? pros and cons on each method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 Lab\n",
    "- **Purpose:** compare the pros and cons between each solving method by implement them.\n",
    "# 2.6.1 Getting toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 2) (200,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "\n",
    "######################################## Data preparation #########################################\n",
    "# PLEASE SET THESE PARAMETERS\n",
    "file = 'https://aegis4048.github.io/downloads/notebooks/sample_data/unconv_MV_v5.csv'\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "X = df[['Por', 'VR']].values.reshape(-1,2)\n",
    "Y = df['Prod']\n",
    "print(X.shape,Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Well</th>\n",
       "      <th>Por</th>\n",
       "      <th>Perm</th>\n",
       "      <th>AI</th>\n",
       "      <th>Brittle</th>\n",
       "      <th>TOC</th>\n",
       "      <th>VR</th>\n",
       "      <th>Prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>100.500000</td>\n",
       "      <td>14.991150</td>\n",
       "      <td>4.330750</td>\n",
       "      <td>2.968850</td>\n",
       "      <td>48.161950</td>\n",
       "      <td>0.990450</td>\n",
       "      <td>1.964300</td>\n",
       "      <td>4311.219852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>57.879185</td>\n",
       "      <td>2.971176</td>\n",
       "      <td>1.731014</td>\n",
       "      <td>0.566885</td>\n",
       "      <td>14.129455</td>\n",
       "      <td>0.481588</td>\n",
       "      <td>0.300827</td>\n",
       "      <td>992.038414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.550000</td>\n",
       "      <td>1.130000</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>10.940000</td>\n",
       "      <td>-0.190000</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>2107.139414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>50.750000</td>\n",
       "      <td>12.912500</td>\n",
       "      <td>3.122500</td>\n",
       "      <td>2.547500</td>\n",
       "      <td>37.755000</td>\n",
       "      <td>0.617500</td>\n",
       "      <td>1.770000</td>\n",
       "      <td>3618.064513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>100.500000</td>\n",
       "      <td>15.070000</td>\n",
       "      <td>4.035000</td>\n",
       "      <td>2.955000</td>\n",
       "      <td>49.510000</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>1.960000</td>\n",
       "      <td>4284.687348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>150.250000</td>\n",
       "      <td>17.402500</td>\n",
       "      <td>5.287500</td>\n",
       "      <td>3.345000</td>\n",
       "      <td>58.262500</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>2.142500</td>\n",
       "      <td>5086.089761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>23.550000</td>\n",
       "      <td>9.870000</td>\n",
       "      <td>4.630000</td>\n",
       "      <td>84.330000</td>\n",
       "      <td>2.180000</td>\n",
       "      <td>2.870000</td>\n",
       "      <td>6662.622385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Well         Por        Perm          AI     Brittle         TOC  \\\n",
       "count  200.000000  200.000000  200.000000  200.000000  200.000000  200.000000   \n",
       "mean   100.500000   14.991150    4.330750    2.968850   48.161950    0.990450   \n",
       "std     57.879185    2.971176    1.731014    0.566885   14.129455    0.481588   \n",
       "min      1.000000    6.550000    1.130000    1.280000   10.940000   -0.190000   \n",
       "25%     50.750000   12.912500    3.122500    2.547500   37.755000    0.617500   \n",
       "50%    100.500000   15.070000    4.035000    2.955000   49.510000    1.030000   \n",
       "75%    150.250000   17.402500    5.287500    3.345000   58.262500    1.350000   \n",
       "max    200.000000   23.550000    9.870000    4.630000   84.330000    2.180000   \n",
       "\n",
       "               VR         Prod  \n",
       "count  200.000000   200.000000  \n",
       "mean     1.964300  4311.219852  \n",
       "std      0.300827   992.038414  \n",
       "min      0.930000  2107.139414  \n",
       "25%      1.770000  3618.064513  \n",
       "50%      1.960000  4284.687348  \n",
       "75%      2.142500  5086.089761  \n",
       "max      2.870000  6662.622385  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6.2 Visualize dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6.3 Analytical Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6.4 Inexact solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE\n",
    "def loss_function(prediction,ground_truth):\n",
    "    \"\"\"\n",
    "    Calculate the MSE loss function\n",
    "    \"\"\"\n",
    "    L = \n",
    "    return L\n",
    "\n",
    "def mini_batch_gradient_descent(X,y,alpha,d, epochs):\n",
    "    \"\"\"\n",
    "    params: given data X, label y, parameter matrix beta, learning step alpha, termination condition d, number of iterations\n",
    "    output: learned beta, loss value history, gradient history\n",
    "    \"\"\"\n",
    "    # Initialize variables beta, epoch, loss_value\n",
    "    beta =\n",
    "    # While loop\n",
    "    while loss_value<d or epoch<epochs:\n",
    "        #calulate loss\n",
    "        \n",
    "        #calculate gradient\n",
    "        \n",
    "        #update beta, epoch\n",
    "    \n",
    "    return beta, loss_history, gradient_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6.5 Comments\n",
    "- Please write up some analysis/comments on the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some nice visualization to provide intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "\n",
    "\n",
    "################################################ Train #############################################\n",
    "\n",
    "ols = linear_model.LinearRegression()\n",
    "model = ols.fit(X, y)\n",
    "response = model.predict(X)\n",
    "\n",
    "############################################## Evaluate ############################################\n",
    "\n",
    "r2 = model.score(X, y)\n",
    "\n",
    "############################################## Plot ################################################\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "ax.plot(X, response, color='k', label='Regression model')\n",
    "ax.scatter(X, y, edgecolor='k', facecolor='grey', alpha=0.7, label='Sample data')\n",
    "ax.set_ylabel('Gas production (Mcf/day)', fontsize=14)\n",
    "ax.set_xlabel('Porosity (%)', fontsize=14)\n",
    "ax.text(0.8, 0.1, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n",
    "         transform=ax.transAxes, color='grey', alpha=0.5)\n",
    "ax.legend(facecolor='white', fontsize=11)\n",
    "ax.set_title('$R^2= %.2f$' % r2, fontsize=18)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax2 = fig.add_subplot(132, projection='3d')\n",
    "ax3 = fig.add_subplot(133, projection='3d')\n",
    "\n",
    "axes = [ax1, ax2, ax3]\n",
    "\n",
    "for ax in axes:\n",
    "    ax.plot(x, y, z, color='k', zorder=15, linestyle='none', marker='o', alpha=0.5)\n",
    "    ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, facecolor=(0,0,0,0), s=20, edgecolor='#70b3f0')\n",
    "    ax.set_xlabel('Porosity (%)', fontsize=12)\n",
    "    ax.set_ylabel('Brittleness', fontsize=12)\n",
    "    ax.set_zlabel('Gas Prod. (Mcf/day)', fontsize=12)\n",
    "    ax.locator_params(nbins=4, axis='x')\n",
    "    ax.locator_params(nbins=5, axis='x')\n",
    "\n",
    "ax1.text2D(0.2, 0.32, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n",
    "           transform=ax1.transAxes, color='grey', alpha=0.5)\n",
    "ax2.text2D(0.3, 0.42, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n",
    "           transform=ax2.transAxes, color='grey', alpha=0.5)\n",
    "ax3.text2D(0.85, 0.85, 'aegis4048.github.io', fontsize=13, ha='center', va='center',\n",
    "           transform=ax3.transAxes, color='grey', alpha=0.5)\n",
    "\n",
    "ax1.view_init(elev=28, azim=120)\n",
    "ax2.view_init(elev=4, azim=114)\n",
    "ax3.view_init(elev=60, azim=165)\n",
    "\n",
    "fig.suptitle('$R^2 = %.2f$' % r2, fontsize=20)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
